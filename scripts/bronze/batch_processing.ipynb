{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde3b516-2eb6-46a7-a5b9-c01197bb6bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Defining functions to monitor processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0a4ddd-2713-4455-9929-095c638a4bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' Assuming there are no modifications to existing files & only new files will be added with new data '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd9f181-89f6-419c-98c4-77e431765c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining get_processed_files function\n",
    "\n",
    "def get_processed_files():\n",
    "    \"\"\"This function is for getting already processed files\"\"\"\n",
    "\n",
    "    try:\n",
    "        processed = spark.sql(\"\"\"\n",
    "                SELECT DISTINCT source_file \n",
    "                FROM workspace.bronze.processed_files \n",
    "            \"\"\").collect()\n",
    "        return set([row.source_file for row in processed])\n",
    "    except:\n",
    "        #no processed files found\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383c9217-4247-4611-97f5-532bae5f9641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining the logger function\n",
    "from datetime import datetime\n",
    "\n",
    "def log_processed_file(filename, row_count):\n",
    "    \"\"\"Log processed file details\"\"\"\n",
    "\n",
    "    log_df = spark.createDataFrame(\n",
    "        [(filename, row_count, datetime.now())],\n",
    "        [\"source_file\", \"row_count\", \"processed_ts\"]\n",
    "    )\n",
    "\n",
    "    log_df.write.mode(\"append\").saveAsTable(\"workspace.bronze.processed_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180f1f0b-cf86-4d81-b28b-68b79ccc5533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Define files path and Start Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b43549-9350-4775-87b2-e879b3a0c30e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_crm_files = dbutils.fs.ls(\"/Volumes/workspace/bronze/sources/source_crm/\")\n",
    "source_erp_files = dbutils.fs.ls(\"/Volumes/workspace/bronze/sources/source_erp/\")\n",
    "\n",
    "csv_crm_files = [file.path for file in source_crm_files if file.path.endswith('.csv')]\n",
    "csv_erp_files = [file.path for file in source_erp_files if file.path.endswith('.csv')]\n",
    "\n",
    "all_csv_files = [*csv_crm_files, *csv_erp_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245d94ff-5343-4332-95cf-df8bbe193023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get already processed files\n",
    "processed_files = get_processed_files()\n",
    "#filter for only new added files to the volume\n",
    "new_files = [file for file in all_csv_files if file not in processed_files]\n",
    "\n",
    "print(f\"Total files: {len(all_csv_files)}, New files: {len(new_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de11b0f-e5d0-4e09-9658-e20166616216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#start processing new files\n",
    "\n",
    "for file_path in new_files:\n",
    "    print(f\"processing new file: {file_path}\")\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    row_count = df.count()\n",
    "\n",
    "    #write to delta tables\n",
    "    table_name = file_path.split('/')[-1].split('.')[0]\n",
    "    df.write.mode(\"append\").saveAsTable(f\"workspace.bronze.{table_name}\")\n",
    "\n",
    "    #log processed file\n",
    "    log_processed_file(file_path, row_count)\n",
    "\n",
    "    print(f\"Processed: {file_path} {row_count} rows\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "batch_processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
