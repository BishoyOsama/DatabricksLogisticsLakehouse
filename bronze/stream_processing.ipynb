{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60486a11-cb3c-404a-b896-1b671c2f0325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup Confluent Kafka (credentials & test connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff16183-531c-4ecd-a440-1ca71b52b68c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get confluent kafka secrets\n",
    "\n",
    "bootstrap_servers = dbutils.secrets.get(scope=\"confluent-kafka\", key=\"bootstrap-servers\")\n",
    "sasl_username = dbutils.secrets.get(scope=\"confluent-kafka\", key=\"api-key\")\n",
    "sasl_password = dbutils.secrets.get(scope=\"confluent-kafka\", key=\"api-secret\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f00d6a-c1c2-4a72-86ef-5b90b18d960c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get Schema Registry secret\n",
    "\n",
    "schema_registry_url = dbutils.secrets.get(scope=\"confluent-kafka\", key=\"schema-registry-url\")\n",
    "schema_registry_api_key = dbutils.secrets.get(scope=\"confluent-kafka\", key=\"schema-registry-api-key\")\n",
    "schema_registry_api_secret = dbutils.secrets.get(scope=\"confluent-kafka\", key=\"schema-registry-api-secret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f7bea9-ca52-420a-b2b6-fb59811ed7a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Other configurations\n",
    "topic = \"orders\"\n",
    "subject = \"orders-value\"\n",
    "bronze_table = \"bronze.orders_stream\"\n",
    "checkpoint_path = \"/Volumes/workspace/bronze/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79382fde-236b-46ea-978d-5edf90529b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from functools import lru_cache\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, DoubleType, BooleanType, ArrayType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298bdf1a-9ba7-4c7f-a3d9-ce6c24afed5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check connectivity\n",
    "\n",
    "''' try:\n",
    "\n",
    "    jaas_config = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(sasl_username, sasl_password)\n",
    "\n",
    "    test_df = (spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", bootstrap_servers)\n",
    "        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "        .option(\"kafka.sasl.jaas.config\", jaas_config)\n",
    "        .option(\"subscribe\", topic)\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    print(\"Connection successful\")\n",
    "\n",
    "except Exception as e:\n",
    "    print( f\"Connection failed: {e}\") '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c2d3c3-5c97-455c-aeb9-55885c78afb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Resolving messages schema from Confluent Schema Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b701da7-d94c-4ed4-b9ac-e5bfba866187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mapping python types to spark types\n",
    "\n",
    "_TYPE_MAP = {\n",
    "    \"string\": StringType(),\n",
    "    \"integer\": IntegerType(),\n",
    "    \"number\": DoubleType(),\n",
    "    \"boolean\": BooleanType()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41c99dd-9a5a-4f88-9365-9723baa72b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _resolve_type(field_def: dict):\n",
    "    \"\"\" Recursivesly converts one JSON schema field to a Pyspark DataType\"\"\"\n",
    "\n",
    "    typ = field_def.get(\"type\", \"string\")\n",
    "    if typ == \"object\":\n",
    "        return _json_schema_to_struct(field_def)\n",
    "    if typ == \"array\":\n",
    "        return ArrayType(_resolve_type(field_def.get(\"items\", {\"type\": \"string\"})))\n",
    "    return _TYPE_MAP.get(typ, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc383bd9-6baf-4223-8281-67b3adb9d8a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _json_schema_to_struct(json_schema: dict) -> StructType:\n",
    "    \"\"\"Converts a full JSON Schema to Spark StructType\"\"\"\n",
    "    required = set(json_schema.get(\"required\", []))\n",
    "    return StructType([\n",
    "        StructField(name, _resolve_type(defn), nullable=(name not in required)) for name, defn in json_schema.get(\"properties\", {}).items()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c436473c-a808-400b-8d33-edc7fd28810b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=32)\n",
    "def fetch_schema(subject: str) -> StructType:\n",
    "    \"\"\"Pulls latest JSON Schema from confluent cloud and returns a StructType\"\"\"\n",
    "\n",
    "    response = requests.get(\n",
    "        f\"{schema_registry_url}/subjects/{subject}/versions/latest\",\n",
    "        auth=(schema_registry_api_key, schema_registry_api_secret),\n",
    "        headers={\"Accept\": \"application/vnd.schemaregistry.v1+json\"}\n",
    "    )\n",
    "\n",
    "    response.raise_for_status()\n",
    "\n",
    "    record= response.json()\n",
    "    json_schema= json.loads(record[\"schema\"])\n",
    "\n",
    "    print(f\"[SR] subject={subject} | version={record['version']} | id={record['id']}\")\n",
    "    print(f\"[SR] properties: {list(json_schema.get('properties', {}).keys())}\")\n",
    "\n",
    "    return _json_schema_to_struct(json_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea435731-381e-4c06-b130-c6dff6656704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Reading Kafka Topic & Write Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9556b217-8ee8-4642-8b13-3c85e0dbc6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    orders_schema: StructType = fetch_schema(subject)\n",
    "    print(f\"\\n[Spark] Using Schema:\\n{orders_schema.simpleString()}\\n\")\n",
    "\n",
    "    jaas_config = \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(sasl_username, sasl_password)\n",
    "\n",
    "    raw_orders_stream = (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", bootstrap_servers)\n",
    "        .option(\"subscribe\", topic)\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "        .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "        .option(\"kafka.sasl.jaas.config\", jaas_config)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    parsed_orders_stream = (\n",
    "        raw_orders_stream\n",
    "        .withColumn(\"value\", F.col(\"value\").cast(\"string\"))\n",
    "        .withColumn(\"data\", F.from_json(\"value\", orders_schema))\n",
    "        .select(\n",
    "            \"data.*\",\n",
    "            F.col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
    "            F.col(\"timestamp\").alias(\"kafka_timestamp\"),\n",
    "            F.col(\"partition\").alias(\"kafka_partition\"),\n",
    "            F.col(\"offset\").alias(\"kafka_offset\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    query = (\n",
    "        parsed_orders_stream.writeStream\n",
    "        .trigger(availableNow=True)\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .table(bronze_table)\n",
    "    )\n",
    "\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49cf55d3-23ed-40a8-ad2d-327c48f355dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stream_processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
